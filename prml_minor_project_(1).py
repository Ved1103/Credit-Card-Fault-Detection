# -*- coding: utf-8 -*-
"""PRML_Minor_Project (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FPol97jKWKpPwn9YfvCT7BfDtxAPGY3w

*   Patel Samarth Rajeshkumar : B21CS057

*   Ved Brahmbhatt : B21EE075

*   Somshuvra Basu : B21EE069

# **Importing Necessary Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, roc_curve, auc

import warnings
warnings.filterwarnings('ignore')

!pip install imblearn

from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from imblearn.metrics import classification_report_imbalanced

"""# **Loading Dataset**

Documentation :
*   As the dataset is too large to load from local machine, it is first uploaded in Google Drive then loaded.
"""

from google.colab import drive
drive.mount('/content/gdrive')

df = pd.read_csv('/content/gdrive/MyDrive/creditcard.csv')

"""# **Data Preprocessing and Exploratory Data Analysis**"""

df.head()

df.shape

"""

*   Checking the datatype and name of all the features.

"""

df.info()

#checking numerical values in dataset
df.describe()

"""

*   Checking if there are any null values in any feature of the dataset.

"""

#checking null values
df.isnull().sum()

print("The columns in the dataset are ", df.columns)

"""

*   Printing the distribution of the classes (Fraud and Non-Fraud).



"""

print("The number of fraud in the dataset are ", df['Class'].value_counts()[1]/len(df) * 100, " %.")
print("The number of non-fraud in the dataset are ", df['Class'].value_counts()[0]/len(df) * 100, " %.")

"""

*   Plotting the number of Fraud and Non-Fraud classes.

"""

plt.hist(df['Class'], bins=3)
plt.title("Class (Fraud=1) (Non-Fraud=0)")
plt.show()

plt.hist(df['Time'])
plt.title("Time hist")
plt.show()
plt.hist(df['Amount'])
plt.title("Amount hist")
plt.show()

"""

*   Checking the distribution of all the features using histograms of all features.

"""

df.hist(figsize=(20, 20))
plt.grid(False)
plt.show()

"""
Documentation :
*   As mentioned in Kaggle website for the dataset, that the Time and Amount features are not scaled, we first scaled them.
*   Robust Scaler for Time because it's distribution is non-gaussian and Standard Scaler for Amount because it appears to be skewed normal from the histograms.

"""

from sklearn.preprocessing import StandardScaler, RobustScaler
standard_scaler = StandardScaler()
robust_scaler = RobustScaler()
standard_scaler.fit(df['Amount'].values.reshape(-1,1))
robust_scaler.fit(df['Time'].values.reshape(-1,1))

df['Amount'] = standard_scaler.transform(df['Amount'].values.reshape(-1,1))
df['Time'] = robust_scaler.transform(df['Time'].values.reshape(-1,1))

df.head()

fig, ax = plt.subplots(1, 2, figsize=(18,4))
amount_val = df['Amount'].values
time_val = df['Time'].values
sns.distplot(amount_val, ax=ax[0], color='r')
ax[0].set_title('Distribution of Transaction Amount', fontsize=14)
ax[0].set_xlim([min(amount_val), max(amount_val)])

sns.distplot(time_val, ax=ax[1], color='b')
ax[1].set_title('Distribution of Transaction Time', fontsize=14)
ax[1].set_xlim([min(time_val), max(time_val)])

plt.show()

"""

*   Plotting the correlation between all the feautures using seaborn library.

"""

correlation=df.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation,cmap='coolwarm',annot=False)
plt.show()

"""

*   Dividing the dataset into dependent (X) and independent (y) variables.
*   Then splitting the dataset into training and testing dataset.

Note : stratify=y is used in train_test_split because we need to maintain the ratio of fraud cases in train and test dataset, as the dataset is higly imbalanced.

"""

y=df['Class']
X=df.drop(['Class'],axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
print("The number of fraud samples in y_train are ", sum(y_train == 1))
print("The number of fraud samples in y_test are ", sum(y_test == 1))

X_test

"""# **Applying Random UnderSampling**

Documentation :


*   First we applied Random Under Sampling, which balances the dataset. It will randomly choose datasamples of majority class to make it equal to minority class.
*   Hence the total datasamples in training dataset is 2*(number of samples in minority class).
*   Random Under Sampling is applied to training dataset and the models are trained using GridSearch CV, which finds optimal values for hyperparameters for the selected models.
*   Then the models are tested on original test dataset.
*   Also, the positive and negative correlation features are visualised after Random Under Sampling is applied to training dataset.
"""

rus = RandomUnderSampler(random_state=42)
X_under_train, y_under_train = rus.fit_resample(X_train, y_train)

df_under = pd.DataFrame(X_under_train, columns=X_train.columns)
df_under['Class'] = y_under_train

df_under.head()

plt.hist(df_under['Class'], bins=3)
plt.title("Class (Fraud=1) (Non-Fraud=0)")
plt.show()

correlation=df_under.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation,cmap='coolwarm',annot=False)
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.boxplot(x="Class", y="V17", data=df_under, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V17 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V14", data=df_under, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V14 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V12", data=df_under, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V12 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V10", data=df_under, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V10 vs Class Negative Correlation')
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)
sns.boxplot(x="Class", y="V11", data=df_under, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V11 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V4", data=df_under, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V4 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V2", data=df_under, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V2 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V19", data=df_under, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V19 vs Class Positive Correlation')
plt.show()

print(X_under_train.shape)
print(y_under_train.shape)

X_under_train = X_under_train.drop(['Class'], axis=1)

from sklearn.model_selection import GridSearchCV

# Define the models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(probability=True)
}

# Define the hyperparameters to tune for each model
params = {
    'Logistic Regression': {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'penalty': ['l1', 'l2', 'elasticnet'],
        'solver': ['liblinear', 'saga']
    },
    'Decision Tree': {
        'max_depth': [5, 10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'KNN': {
        'n_neighbors': [3, 5, 7, 9, 11],
        'weights': ['uniform', 'distance'],
        'algorithm': ['ball_tree', 'kd_tree', 'brute']
    },
    'SVM': {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
        'degree': [2, 3, 4],
        'gamma': ['scale', 'auto']
    }
}

# Fit each model using GridSearchCV to find the best hyperparameters
best_models = {}
for name, model in models.items():
    clf = GridSearchCV(model, params[name], cv=5, n_jobs=-1)
    clf.fit(X_under_train, y_under_train)
    best_models[name] = clf.best_estimator_
    print(f"Best hyperparameters for {name}: {clf.best_params_}")

# Evaluate the best models on the testing set
results = {}
for name, model in best_models.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    results[name] = {
        'Accuracy': accuracy,
        'F1 score': f1,
        'Precision': precision,
        'Recall': recall,
        'ROC AUC': roc_auc,
        'FPR': fpr,
        'TPR': tpr,
        'Thresholds': thresholds
    }

"""

*   After the models are trained (may take few minutes), various metrics for each model are tested and stored in dataframe to compare.
*   The ROC-AUC curve is also plotted for each model and then we find the confusion matrix for the testing as well as training dataset.
*   Seeing all these parameters and metrics, best model for Random Under Sampling is selected.



"""

# Create a DataFrame to store the results
df_results = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 score', 'Precision', 'Recall', 'ROC AUC'])

# Add the results for each model to the DataFrame
for name, metrics in results.items():
  accuracy = metrics['Accuracy']
  f1 = metrics['F1 score']
  precision = metrics['Precision']
  recall = metrics['Recall']
  roc_auc = metrics['ROC AUC']
  df_results = df_results.append({
    'Model': name,
    'Accuracy': accuracy,
    'F1 score': f1,
    'Precision': precision,
    'Recall': recall,
    'ROC AUC': roc_auc
  }, ignore_index=True)

df_results

# Create a grid of subplots for ROC curves
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))

# Plot ROC curves for each model on a separate subplot
for ax, (name, metrics) in zip(axes.flat, results.items()):
  fpr = metrics['FPR']
  tpr = metrics['TPR']
  roc_auc = metrics['ROC AUC']
  ax.plot([0, 1], [0, 1], linestyle='--')
  ax.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")
  ax.set_xlabel('False Positive Rate')
  ax.set_ylabel('True Positive Rate')
  ax.set_title(f'ROC Curve for {name}')
  ax.legend()
# Adjust spacing between subplots
plt.tight_layout()
# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix

class_names = ['Non-Fraud', 'Fraud']

# Define the models and their names
models = [best_models['Logistic Regression'],
          best_models['Decision Tree'],
          best_models['Random Forest'],
          best_models['KNN'],
          best_models['SVM']]
model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN', 'SVM']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names, models)):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    ax = axes[i//3][i%3]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names, models)):
    y_pred = model.predict(X_under_train)
    cm = confusion_matrix(y_under_train, y_pred)
    ax = axes[i//3][i%3]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

"""Results of Random Under Sampling :


*   Random Forest gives best confusion matrix but the F1 score is not good as the precision is low for it.
*   Best model for Random Under Sampling is Random Forest.

# **Applying Random OverSampling**

Documentation :

*   First we applied Random Over Sampling, which balances the dataset. It will randomly choose datasamples of minority class with replacement to make it equal to majority class.
*   Hence the total datasamples in training dataset is 2*(number of samples in majority class).
*   Random Over Sampling is applied to training dataset and the models are trained.
*   Then the models are tested on original test dataset.
*   Also, the positive and negative correlation features are visualised after Random Over Sampling is applied to training dataset.
"""

# Perform random oversampling
ros = RandomOverSampler(random_state=42)
X_over_train, y_over_train = ros.fit_resample(X_train, y_train)

# Create new DataFrame of resampled data
df_over = pd.concat([pd.DataFrame(X_over_train), pd.DataFrame(y_over_train)], axis=1)
df_over.columns = df.columns

plt.hist(y_over_train, bins=3)
plt.title("Class (Fraud=1) (Non-Fraud=0)")
plt.show()

correlation=df_over.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation,cmap='coolwarm',annot=False)
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.boxplot(x="Class", y="V17", data=df_over, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V17 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V14", data=df_over, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V14 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V12", data=df_over, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V12 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V10", data=df_over, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V10 vs Class Negative Correlation')
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)
sns.boxplot(x="Class", y="V11", data=df_over, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V11 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V4", data=df_over, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V4 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V2", data=df_over, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V2 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V19", data=df_over, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V19 vs Class Positive Correlation')
plt.show()

print(X_over_train.shape)
print(y_over_train.shape)

# Define the models
models_over = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    # 'SVM': SVC(probability=True)
}

best_models_over = {}
for name, model in models_over.items():
    clf = model
    clf.fit(X_over_train, y_over_train)
    best_models_over[name] = clf
    print(f"Model for {name} built.")

# Evaluate the best models on the testing set
results_over = {}
for name, model in best_models_over.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    results_over[name] = {
        'Accuracy': accuracy,
        'F1 score': f1,
        'Precision': precision,
        'Recall': recall,
        'ROC AUC': roc_auc,
        'FPR': fpr,
        'TPR': tpr,
        'Thresholds': thresholds
    }

"""*   After the models are trained (may take few minutes or hours), various metrics for each model are tested and stored in dataframe to compare.
*   The ROC-AUC curve is also plotted for each model and then we find the confusion matrix for the testing as well as training dataset.
*   Seeing all these parameters and metrics, best model for Random Over Sampling is selected.
"""

# Create a DataFrame to store the results
df_results_over = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 score', 'Precision', 'Recall', 'ROC AUC'])

# Add the results for each model to the DataFrame
for name, metrics in results_over.items():
  accuracy = metrics['Accuracy']
  f1 = metrics['F1 score']
  precision = metrics['Precision']
  recall = metrics['Recall']
  roc_auc = metrics['ROC AUC']
  df_results_over = df_results_over.append({
    'Model': name,
    'Accuracy': accuracy,
    'F1 score': f1,
    'Precision': precision,
    'Recall': recall,
    'ROC AUC': roc_auc
  }, ignore_index=True)

df_results_over

# Create a grid of subplots for ROC curves
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))

# Plot ROC curves for each model on a separate subplot
for ax, (name, metrics) in zip(axes.flat, results_over.items()):
  fpr = metrics['FPR']
  tpr = metrics['TPR']
  roc_auc = metrics['ROC AUC']
  ax.plot([0, 1], [0, 1], linestyle='--')
  ax.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")
  ax.set_xlabel('False Positive Rate')
  ax.set_ylabel('True Positive Rate')
  ax.set_title(f'ROC Curve for {name}')
  ax.legend()
# Adjust spacing between subplots
plt.tight_layout()
# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix

class_names = ['Non-Fraud', 'Fraud']

# Define the models and their names
models_over_ = [best_models_over['Logistic Regression'],
          best_models_over['Decision Tree'],
          best_models_over['Random Forest'],
          best_models_over['KNN']]
model_names_over = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names_over, models_over_)):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    ax = axes[i//2][i%2]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names_over, models_over_)):
    y_pred = model.predict(X_over_train)
    cm = confusion_matrix(y_over_train, y_pred)
    ax = axes[i//2][i%2]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

"""Results of Random Over Sampling :

*   Random Forest gives best confusion matrix and the F1 score is also pretty good as the precision and Recall are good for it.
*   Best model for Random Over Sampling is Random Forest.

# **Applying SMOTE OverSampling**

SMOTE (Synthetic Minority Over-sampling Technique) is another technique for oversampling the minority class. It creates new synthetic instances of the minority class by creating new instances along the line segments that join the minority class instances in the feature space.

The algorithm works as follows:

*   For each minority class instance, find its k nearest minority class neighbors (k_neighbors).
*   Choose one of the k_neighbors randomly and compute the difference between the feature vector of the chosen neighbor and the current minority class instance (diff = chosen_neighbor - current_instance).
*   Multiply this difference by a random number between 0 and 1 (a random ratio) and add the result to the current instance to generate a new synthetic instance.

By repeating these steps for each minority class instance, we can generate as many synthetic minority class instances as required.

This technique helps to balance the class distribution without overfitting.

The synthetic instances generated by SMOTE are not just duplicates of the minority class instances, but rather new examples that reflect the underlying distribution of the minority class.

This helps to improve the generalization of the model.

Documentation :

*   First we applied SMOTE Over Sampling, which balances the dataset.
*   SMOTE Over Sampling is applied to training dataset and the models are trained.
*   Then the models are tested on original test dataset.
*   Also, the positive and negative correlation features are visualised after SMOTE Over Sampling is applied to training dataset.
"""

smote = SMOTE(random_state=42)
X_smote_train, y_smote_train = smote.fit_resample(X_train, y_train)

# Create new DataFrame of resampled data
df_smote = pd.concat([pd.DataFrame(X_smote_train), pd.DataFrame(y_smote_train)], axis=1)
df_smote.columns = df.columns

plt.hist(y_smote_train, bins=3)
plt.title("Class (Fraud=1) (Non-Fraud=0)")
plt.show()

correlation=df_smote.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation,cmap='coolwarm',annot=False)
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.boxplot(x="Class", y="V17", data=df_smote, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V17 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V14", data=df_smote, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V14 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V12", data=df_smote, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V12 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V10", data=df_smote, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V10 vs Class Negative Correlation')
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)
sns.boxplot(x="Class", y="V11", data=df_smote, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V11 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V4", data=df_smote, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V4 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V2", data=df_smote, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V2 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V19", data=df_smote, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V19 vs Class Positive Correlation')
plt.show()

print(X_smote_train.shape)
print(y_smote_train.shape)

# Define the models
models_smote = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    # 'SVM': SVC(probability=True)
}

best_models_smote = {}
for name, model in models_smote.items():
    clf = model
    clf.fit(X_smote_train, y_smote_train)
    best_models_smote[name] = clf
    print(f"Model for {name} built.")

# Evaluate the best models on the testing set
results_smote = {}
for name, model in best_models_smote.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    results_smote[name] = {
        'Accuracy': accuracy,
        'F1 score': f1,
        'Precision': precision,
        'Recall': recall,
        'ROC AUC': roc_auc,
        'FPR': fpr,
        'TPR': tpr,
        'Thresholds': thresholds
    }

"""*   After the models are trained (may take few minutes or hours), various metrics for each model are tested and stored in dataframe to compare.
*   The ROC-AUC curve is also plotted for each model and then we find the confusion matrix for the testing as well as training dataset.
*   Seeing all these parameters and metrics, best model for Random Over Sampling is selected.
"""

# Create a DataFrame to store the results
df_results_smote = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 score', 'Precision', 'Recall', 'ROC AUC'])

# Add the results for each model to the DataFrame
for name, metrics in results_smote.items():
  accuracy = metrics['Accuracy']
  f1 = metrics['F1 score']
  precision = metrics['Precision']
  recall = metrics['Recall']
  roc_auc = metrics['ROC AUC']
  df_results_smote = df_results_smote.append({
    'Model': name,
    'Accuracy': accuracy,
    'F1 score': f1,
    'Precision': precision,
    'Recall': recall,
    'ROC AUC': roc_auc
  }, ignore_index=True)

df_results_smote

# Create a grid of subplots for ROC curves
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))

# Plot ROC curves for each model on a separate subplot
for ax, (name, metrics) in zip(axes.flat, results_smote.items()):
  fpr = metrics['FPR']
  tpr = metrics['TPR']
  roc_auc = metrics['ROC AUC']
  ax.plot([0, 1], [0, 1], linestyle='--')
  ax.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")
  ax.set_xlabel('False Positive Rate')
  ax.set_ylabel('True Positive Rate')
  ax.set_title(f'ROC Curve for {name}')
  ax.legend()
# Adjust spacing between subplots
plt.tight_layout()
# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix

class_names = ['Non-Fraud', 'Fraud']

# Define the models and their names
models_smote_ = [best_models_smote['Logistic Regression'],
          best_models_smote['Decision Tree'],
          best_models_smote['Random Forest'],
          best_models_smote['KNN']]
model_names_smote = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names_smote, models_smote_)):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    ax = axes[i//2][i%2]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names_smote, models_smote_)):
    y_pred = model.predict(X_smote_train)
    cm = confusion_matrix(y_smote_train, y_pred)
    ax = axes[i//2][i%2]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

"""Results of SMOTE Over Sampling :

*   Random Forest gives best confusion matrix and the F1 score is also pretty good as the precision and Recall are good for it.
*   Best model for SMOTE Over Sampling is Random Forest.
*   SMOTE results are nearly similar to Random Over Sampling.

# **Applying NearMiss UnderSampling**

*   NearMiss is a family of under-sampling techniques which selects the samples from the majority class whose distances to the closest samples in the minority class are the smallest.
*   The main idea behind NearMiss is that samples from the majority class that are located close to the minority class are more likely to be misclassified than those that are far away from the minority class.
*   The selected samples from the majority class are kept in the final dataset, and the minority class is left unchanged.
*   Therefore, NearMiss under-samples the majority class and does not create new synthetic samples like the over-sampling techniques do.

Documentation :

*   First we applied NearMiss Under Sampling, which balances the dataset.
*   NearMiss Under Sampling is applied to training dataset and the models are trained using GridSearch CV, which finds optimal values for hyperparameters for the selected models.
*   Then the models are tested on original test dataset.
*   Also, the positive and negative correlation features are visualised after NearMiss Under Sampling is applied to training dataset.
"""

# apply NearMiss to the training dataset
nm = NearMiss()
X_near_train, y_near_train = nm.fit_resample(X_train, y_train)

# Create new DataFrame of resampled data
df_nearMiss = pd.concat([pd.DataFrame(X_near_train), pd.DataFrame(y_near_train)], axis=1)
df_nearMiss.columns = df.columns

plt.hist(y_near_train, bins=3)
plt.title("Class (Fraud=1) (Non-Fraud=0)")
plt.show()

correlation=df_nearMiss.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation,cmap='coolwarm',annot=False)
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.boxplot(x="Class", y="V17", data=df_nearMiss, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V17 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V14", data=df_nearMiss, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V14 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V12", data=df_nearMiss, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V12 vs Class Negative Correlation')
sns.boxplot(x="Class", y="V10", data=df_nearMiss, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V10 vs Class Negative Correlation')
plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))
# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)
sns.boxplot(x="Class", y="V11", data=df_nearMiss, palette=['b', 'r'], ax=axes[0])
axes[0].set_title('V11 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V4", data=df_nearMiss, palette=['b', 'r'], ax=axes[1])
axes[1].set_title('V4 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V2", data=df_nearMiss, palette=['b', 'r'], ax=axes[2])
axes[2].set_title('V2 vs Class Positive Correlation')
sns.boxplot(x="Class", y="V19", data=df_nearMiss, palette=['b', 'r'], ax=axes[3])
axes[3].set_title('V19 vs Class Positive Correlation')
plt.show()

print(X_near_train.shape)
print(y_near_train.shape)

from sklearn.model_selection import GridSearchCV

# Define the models
models_nearMiss = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(probability=True)
}

# Define the hyperparameters to tune for each model
params = {
    'Logistic Regression': {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'penalty': ['l1', 'l2', 'elasticnet'],
        'solver': ['liblinear', 'saga']
    },
    'Decision Tree': {
        'max_depth': [5, 10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'KNN': {
        'n_neighbors': [3, 5, 7, 9, 11],
        'weights': ['uniform', 'distance'],
        'algorithm': ['ball_tree', 'kd_tree', 'brute']
    },
    'SVM': {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
        'degree': [2, 3, 4],
        'gamma': ['scale', 'auto']
    }
}

# Fit each model using GridSearchCV to find the best hyperparameters
best_models_nearMiss = {}
for name, model in models_nearMiss.items():
    clf = GridSearchCV(model, params[name], cv=5, n_jobs=-1)
    clf.fit(X_near_train, y_near_train)
    best_models_nearMiss[name] = clf.best_estimator_
    print(f"Best hyperparameters for {name}: {clf.best_params_}")

# Evaluate the best models on the testing set
results_nearMiss = {}
for name, model in best_models_nearMiss.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    results_nearMiss[name] = {
        'Accuracy': accuracy,
        'F1 score': f1,
        'Precision': precision,
        'Recall': recall,
        'ROC AUC': roc_auc,
        'FPR': fpr,
        'TPR': tpr,
        'Thresholds': thresholds
    }

"""*   After the models are trained (may take few minutes or hours), various metrics for each model are tested and stored in dataframe to compare.
*   The ROC-AUC curve is also plotted for each model and then we find the confusion matrix for the testing as well as training dataset.
*   Seeing all these parameters and metrics, best model for Random Over Sampling is selected.
"""

# Create a DataFrame to store the results
df_results_nearMiss = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 score', 'Precision', 'Recall', 'ROC AUC'])

# Add the results for each model to the DataFrame
for name, metrics in results_nearMiss.items():
  accuracy = metrics['Accuracy']
  f1 = metrics['F1 score']
  precision = metrics['Precision']
  recall = metrics['Recall']
  roc_auc = metrics['ROC AUC']
  df_results_nearMiss = df_results_nearMiss.append({
    'Model': name,
    'Accuracy': accuracy,
    'F1 score': f1,
    'Precision': precision,
    'Recall': recall,
    'ROC AUC': roc_auc
  }, ignore_index=True)

df_results_nearMiss

# Create a grid of subplots for ROC curves
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))

# Plot ROC curves for each model on a separate subplot
for ax, (name, metrics) in zip(axes.flat, results_nearMiss.items()):
  fpr = metrics['FPR']
  tpr = metrics['TPR']
  roc_auc = metrics['ROC AUC']
  ax.plot([0, 1], [0, 1], linestyle='--')
  ax.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")
  ax.set_xlabel('False Positive Rate')
  ax.set_ylabel('True Positive Rate')
  ax.set_title(f'ROC Curve for {name}')
  ax.legend()
# Adjust spacing between subplots
plt.tight_layout()
# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix

class_names = ['Non-Fraud', 'Fraud']

# Define the models and their names
models_nearMiss = [best_models_nearMiss['Logistic Regression'],
          best_models_nearMiss['Decision Tree'],
          best_models_nearMiss['Random Forest'],
          best_models_nearMiss['KNN'],
          best_models_nearMiss['SVM']]
model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN', 'SVM']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names, models_nearMiss)):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    ax = axes[i//3][i%3]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names, models_nearMiss)):
    y_pred = model.predict(X_near_train)
    cm = confusion_matrix(y_near_train, y_pred)
    ax = axes[i//3][i%3]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

"""Results of NearMiss Under Sampling :

*   Logistic Regression gives best confusion matrix and the F1 score is also pretty good as the precision and Recall are good for it.
*   Best model for NearMiss Under Sampling is Logistic Regression.
*   NearMiss results are not similar to Random Under Sampling, it performs a bit better than Random Under Sampling.

# **Analysing Best Models**

*   Combining all the best models from all the 4 techniques applied for balancing the dataset.
*   Storing their results in a new dataframe.
"""

best_models = {
    'Random Forest_RandomUnderSampling' : models[2],
    'Random Forest_RandomOverSampling' : models_over['Random Forest'],
    'Random Forest_SMOTE' : models_smote['Random Forest'],
    'Logistic Regression_NearMiss' : models_nearMiss[0]
}

best_models

# select best model's metrics from each dataframe
df_under_selected = df_results.loc[df_results['Model'] == 'Random Forest']
df_over_selected = df_results_over.loc[df_results_over['Model'] == 'Random Forest']
df_smote_selected = df_results_smote.loc[df_results_smote['Model'] == 'Random Forest']
df_nearMiss_selected = df_results_nearMiss.loc[df_results_nearMiss['Model'] == 'Logistic Regression']

# concatenate the selected rows into a new dataframe
df_best_models = pd.concat([df_under_selected, df_over_selected, df_smote_selected, df_nearMiss_selected])

df_best_models

"""# **Applying LDA to all Best Models**

NOTE : This is just for experimenting LDA on the best models which are obtained using all 4 data balancing techniques.

*   We apply LDA transformation on the original dataset anf then train all the best models which we obtained above on it after applying the data balancing techniques for respective models.
*   The results are stores in a dataframe to compare it with dataset, which did not have LDA applied on it.
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Create an LDA object
lda = LinearDiscriminantAnalysis()
# Fit the LDA model on X and y
lda.fit(X, y)
# Transform X to the LDA space
X_lda = lda.transform(X)

X_lda_train, X_lda_test, y_lda_train, y_lda_test = train_test_split(X_lda, y, stratify=y, random_state=42, test_size=0.2)
print("The number of fraud samples in y_lda_train are ", sum(y_lda_train == 1))
print("The number of fraud samples in y_lda_test are ", sum(y_lda_test == 1))

X_lda

rus_lda = RandomUnderSampler(random_state=42)
X_under_lda_train, y_under_lda_train = rus_lda.fit_resample(X_lda_train, y_lda_train)

df_under_lda = pd.DataFrame(X_under_lda_train, columns=['LD1'])
df_under_lda['Class'] = y_under_lda_train

df_under_lda.head()

ros_lda = RandomOverSampler(random_state=42)
X_over_lda_train, y_over_lda_train = ros_lda.fit_resample(X_lda_train, y_lda_train)

df_over_lda = pd.DataFrame(X_over_lda_train, columns=['LD1'])
df_over_lda['Class'] = y_over_lda_train

df_over_lda.head()

smote_lda = SMOTE(random_state=42)
X_smote_lda_train, y_smote_lda_train = smote_lda.fit_resample(X_lda_train, y_lda_train)

df_smote_lda = pd.DataFrame(X_smote_lda_train, columns=['LD1'])
df_smote_lda['Class'] = y_smote_lda_train

df_smote_lda.head()

nearMiss_lda = NearMiss()
X_near_lda_train, y_near_lda_train = nearMiss_lda.fit_resample(X_lda_train, y_lda_train)

df_near_lda = pd.DataFrame(X_near_lda_train, columns=['LD1'])
df_near_lda['Class'] = y_near_lda_train

df_near_lda.head()

lda_models = {
    'Random Forest_RandomUnderSampling_LDA' : RandomForestClassifier(max_depth=10, min_samples_split=5, n_estimators=200),
    'Random Forest_RandomOverSampling_LDA' : RandomForestClassifier(),
    'Random Forest_SMOTE_LDA' : RandomForestClassifier(),
    'Logistic Regression_NearMiss_LDA' : LogisticRegression(C=1, penalty='l1', solver='saga')
}

results_lda = {}
for name, model in lda_models.items():
  if 'RandomUnderSampling' in name:
    X_train_lda = X_under_lda_train
    y_train_lda = y_under_lda_train
  elif 'RandomOverSampling' in name:
    X_train_lda = X_over_lda_train
    y_train_lda = y_over_lda_train
  elif 'SMOTE' in name:
    X_train_lda = X_smote_lda_train
    y_train_lda = y_smote_lda_train
  else:
    X_train_lda = X_near_lda_train
    y_train_lda = y_near_lda_train

  model.fit(X_train_lda, y_train_lda)
  print(f"Model {name} trained.")
  y_pred = model.predict(X_lda_test)
  y_prob = model.predict_proba(X_lda_test)[:, 1]
  accuracy = accuracy_score(y_lda_test, y_pred)
  precision = precision_score(y_lda_test, y_pred)
  recall = recall_score(y_lda_test, y_pred)
  f1 = f1_score(y_lda_test, y_pred)
  roc_auc = roc_auc_score(y_lda_test, y_prob)

  results_lda[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1, 'ROC-AUC': roc_auc}

results_lda_df = pd.DataFrame(results_lda).T

results_lda_df

df_best_models

"""*   Some improvement on the Random Forest of SMOTE can be seen but it is not significant as we can observe that the LDA only gave one Linear Discriminant after the transformation.
*   So we can say that not much information was able to be regained from the original dataset.

NOTE : PCA tranformation is not explicitly not applied because it was mentioned in the Kaggle website of dataset that PCA is already applied on original dataset to obtain features except Time and Amount feature.
*   Hence Standard Scaler and Robust Scaler were applied on Time and Amount features.
"""